<?xml version="1.0" encoding="UTF-8"?>
 <external_stories type="array">
   <external_story>
     <!--https://github.com/vmware-tanzu/pinniped/issues/371-->
     <external_id>371</external_id>
     <name>Do we need to remove `TokenCredentialRequest` from `pinniped` category?</name>
     <description>&lt;!--&#xD;&#xA;&#xD;&#xA;Hey! Thanks for opening an issue!&#xD;&#xA;&#xD;&#xA;IMPORTANT: If you believe this bug is a security issue, please don&#39;t use this template and follow our [security guidelines](/doc/security.md).&#xD;&#xA;&#xD;&#xA;It is recommended that you include screenshots and logs to help everyone achieve a shared understanding of the bug.&#xD;&#xA;&#xD;&#xA;--&gt;&#xD;&#xA;&#xD;&#xA;**What happened?**&#xD;&#xA;&#xD;&#xA;- I am running the Concierge with a `JWTAuthenticator` in the `pinniped-concierge` namespace.&#xD;&#xA;- I ran `kubectl get pinniped` and got this:&#xD;&#xA;```&#xD;&#xA;akeesler@akeesler-a02:pinniped-ci$ k get pinniped&#xD;&#xA;Error from server (MethodNotAllowed): the server does not allow this method on the requested resource&#xD;&#xA;```&#xD;&#xA;- I ran `kubectl get pinniped -n pinniped-concierge` and got this:&#xD;&#xA;```&#xD;&#xA;akeesler@akeesler-a02:pinniped-ci$ k get pinniped -n pinniped-concierge&#xD;&#xA;NAME                                                                           ISSUER&#xD;&#xA;jwtauthenticator.authentication.concierge.pinniped.dev/tkg-jwt-authenticator   https://af0c3cd46a7c2415c835317675239b96-1968935863.us-east-1.elb.amazonaws.com&#xD;&#xA;&#xD;&#xA;NAME                                                                       AGE&#xD;&#xA;credentialissuer.config.concierge.pinniped.dev/pinniped-concierge-config   14m&#xD;&#xA;Error from server (MethodNotAllowed): the server does not allow this method on the requested resource&#xD;&#xA;```&#xD;&#xA;- I ran `kubectl get pinniped -A` and got this:&#xD;&#xA;```&#xD;&#xA;akeesler@akeesler-a02:pinniped-ci$ k get pinniped -A&#xD;&#xA;NAMESPACE            NAME                                                                           ISSUER&#xD;&#xA;pinniped-concierge   jwtauthenticator.authentication.concierge.pinniped.dev/tkg-jwt-authenticator   https://af0c3cd46a7c2415c835317675239b96-1968935863.us-east-1.elb.amazonaws.com&#xD;&#xA;&#xD;&#xA;NAMESPACE            NAME                                                                       AGE&#xD;&#xA;pinniped-concierge   credentialissuer.config.concierge.pinniped.dev/pinniped-concierge-config   15m&#xD;&#xA;Error from server (NotFound): Unable to list &#34;login.concierge.pinniped.dev/v1alpha1, Resource=tokencredentialrequests&#34;: the server could not find the requested resource&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;**What did you expect to happen?**&#xD;&#xA;&#xD;&#xA;- I don&#39;t want to see those `Error`&#39;s show up when I `get` the `pinniped` categories&#xD;&#xA;&#xD;&#xA;**What is the simplest way to reproduce this behavior?**&#xD;&#xA;&#xD;&#xA;- `cd` into the `pinniped` repo&#xD;&#xA;- `./hack/prepare-for-integration-tests.sh`&#xD;&#xA;- Do this:&#xD;&#xA;```&#xD;&#xA;cat &lt;&lt;EOF | kubectl apply -f -&#xD;&#xA;kind: JWTAuthenticator&#xD;&#xA;apiVersion: authentication.concierge.pinniped.dev/v1alpha1&#xD;&#xA;metadata:&#xD;&#xA;  name: whatever&#xD;&#xA;  namespace: concierge&#xD;&#xA;spec:&#xD;&#xA;  issuer: https://whatever.tld&#xD;&#xA;  audience: whatever&#xD;&#xA;EOF&#xD;&#xA;```&#xD;&#xA;- `kubectl get pinniped -A` (you should see an `Error`)&#xD;&#xA;- `kubectl get pinniped -n concierge` (you should see an `Error`)&#xD;&#xA;- `kubectl get pinniped` (you should see an `Error`)&#xD;&#xA;&#xD;&#xA;**In what environment did you see this bug?**&#xD;&#xA;- Pinniped server version: `v0.4.1`&#xD;&#xA;- Pinniped client version: N/A&#xD;&#xA;- Pinniped container image (if using a public container image): `v0.4.1`&#xD;&#xA;- Pinniped configuration (what IDP(s) are you using? what downstream credential minting mechanisms are you using?): `v0.4.1`&#xD;&#xA;- Kubernetes version (use `kubectl version`): `0.20.1`&#xD;&#xA;- Kubernetes installer &amp; version (e.g., `kubeadm version`): N/A&#xD;&#xA;- Cloud provider or hardware configuration: `kind` (`docker`)&#xD;&#xA;- OS (e.g: `cat /etc/os-release`): macOS&#xD;&#xA;- Kernel (e.g. `uname -a`): macOS&#xD;&#xA;- Others:&#xD;&#xA;</description>
     <requested_by>ankeesler</requested_by>
     <story_type>bug</story_type>
     <created_at>2021-01-28T15:35:00Z</created_at>
   </external_story>
   <external_story>
     <!--https://github.com/vmware-tanzu/pinniped/issues/368-->
     <external_id>368</external_id>
     <name>Add concierge impersonation proxy support to `pinniped get kubeconfig` CLI command.</name>
     <description>### Acceptance Criteria&#xD;&#xA;&#xD;&#xA;```gherkin&#xD;&#xA;Scenario: use concierge via the `pinniped get kubeconfig` CLI subcommand.&#xD;&#xA;  Given that I have an managed cluster with the Pinniped concierge installed&#xD;&#xA;    And that I have configured the impersonation proxy appropriately&#xD;&#xA;  When I run `pinniped get kubeconfig`&#xD;&#xA;  Then I can use that kubeconfig to run kubectl commands as my user&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;### Notes&#xD;&#xA;This is a followup to #339, #363, #364, and #366. It covers the `pinniped get kubeconfig` subcommand and builds on the previous changes to the `pinniped login` subcommands.&#xD;&#xA;&#xD;&#xA;### CLI Changes&#xD;&#xA;&#xD;&#xA;There are a few new flags to be added to the `pinniped get kubeconfig ` command:&#xD;&#xA;&#xD;&#xA;1. `--concierge-endpoint` (specifies the endpoint URL of the concierge impersonation proxy).&#xD;&#xA;&#xD;&#xA;2. `--concierge-ca-bundle` (specifies the CA bundle for talking to the concierge).&#xD;&#xA;&#xD;&#xA;3. `--concierge-use-impersonation-proxy` (species that the concierge should be used in impersonation proxy mode).&#xD;&#xA;&#xD;&#xA;Each of these flags can also be defaulted based on the CredentialIssuer found in the target cluster:&#xD;&#xA;&#xD;&#xA;- The `--concierge-use-impersonation-proxy` flag should be set based on the currently successful strategies found in the CredentialIssuer status. If the `KubeClusterSigningCertificate` strategy is failing (as it will on managed cluster environments), then the `--concierge-use-impersonation-proxy` should be defaulted to &#34;true&#34;.&#xD;&#xA;&#xD;&#xA;- The `--concierge-ca-bundle` and `--concierge-ca-bundle`  flags should default to the corresponding `status.impersonationProxy` fields added in #364.&#xD;&#xA;&#xD;&#xA;When the  `--concierge-use-impersonation-proxy` flag is set to true (explicitly or via auto defaulting), then the generated kubeconfig will have some important changes:&#xD;&#xA;&#xD;&#xA;- The `clusters[].cluster.server` and `clusters[].cluster.certificate-authority-data` fields should be set to point at the impersonation proxy.&#xD;&#xA;- The `--enable-concierge-impersonation-proxy` flag (from #366) should be set to true.&#xD;&#xA;</description>
     <requested_by>mattmoyer</requested_by>
     <story_type>feature</story_type>
     <created_at>2021-01-27T21:53:02Z</created_at>
   </external_story>
   <external_story>
     <!--https://github.com/vmware-tanzu/pinniped/issues/228-->
     <external_id>228</external_id>
     <name>The `TestSupervisorLogin` integration test can be flaky.</name>
     <description>**What happened?**&#xD;&#xA;&#xD;&#xA;The `TestSupervisorLogin` test failed on [a PR CI test run](https://hush-house.pivotal.io/builds/9006057):&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;=== RUN   TestSupervisorLogin&#xD;&#xA;    supervisor_login_test.go:41: created test OIDCProvider supervisor/test-oidc-provider-zrtnr&#xD;&#xA;    supervisor_login_test.go:79: created test client credentials Secret test-client-creds-fthnc&#xD;&#xA;    supervisor_login_test.go:82: created test UpstreamOIDCProvider test-upstream-v7p7j&#xD;&#xA;    supervisor_login_test.go:92: &#xD;&#xA;        &#x9;Error Trace:&#x9;supervisor_login_test.go:92&#xD;&#xA;        &#x9;Error:      &#x9;Not equal: &#xD;&#xA;        &#x9;            &#x9;expected: 302&#xD;&#xA;        &#x9;            &#x9;actual  : 422&#xD;&#xA;        &#x9;Test:       &#x9;TestSupervisorLogin&#xD;&#xA;    supervisor_login_test.go:41: cleaning up test OIDCProvider supervisor/test-oidc-provider-zrtnr&#xD;&#xA;--- FAIL: TestSupervisorLogin (1.96s)&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;**What did you expect to happen?**&#xD;&#xA;&#xD;&#xA;The test should succeed!&#xD;&#xA;&#xD;&#xA;**What is the simplest way to reproduce this behavior?**&#xD;&#xA;&#xD;&#xA;I think we should be able to reliably reproduce this flake if we add an artificial delay to the `upstream-observer` controller sync method.&#xD;&#xA;&#xD;&#xA;**In what environment did you see this bug?**&#xD;&#xA;&#xD;&#xA;This occurred on the PR tests for commit ad1bc6c36cbdfdbe9ec19a3da3394f185a095051.&#xD;&#xA;&#xD;&#xA;**What else is there to know about this bug?**&#xD;&#xA;&#xD;&#xA;We can probably fix this by adding the appropriate `require.Eventually(...)` call to the assertion block that&#39;s failing.&#xD;&#xA;</description>
     <requested_by>mattmoyer</requested_by>
     <story_type>feature</story_type>
     <created_at>2020-11-18T21:03:44Z</created_at>
   </external_story>
   <external_story>
     <!--https://github.com/vmware-tanzu/pinniped/issues/348-->
     <external_id>348</external_id>
     <name>Enable audit logging for all of our test environments</name>
     <description>&lt;!--&#xD;&#xA;&#xD;&#xA;Hey! Thanks for opening an issue!&#xD;&#xA;&#xD;&#xA;It is recommended that you include screenshots and logs to help everyone achieve a shared understanding of the improvement.&#xD;&#xA;&#xD;&#xA;--&gt;&#xD;&#xA;&#xD;&#xA;**Is your feature request related to a problem? Please describe.**&#xD;&#xA;A clear and concise description of what the problem is. Ex. I&#39;m always frustrated when [...]&#xD;&#xA;&#xD;&#xA;- @enj and I were debugging a mysteriously deleted `Secret`, and we had a really hard time figuring out why it was getting deleted.&#xD;&#xA;- We enabled audit logging, and immediately discovered what entity was deleting the `Secret` and we were able to figure out our bug.&#xD;&#xA;- More generally: it would be helpful when debugging test environments to have an audit log to help us understand what is going on.&#xD;&#xA;&#xD;&#xA;**Describe the solution you&#39;d like**&#xD;&#xA;A clear and concise description of what you want to happen.&#xD;&#xA;&#xD;&#xA;- Enable `kube-apiserver` audit logs in our test environments (i.e., our test kind clusters).&#xD;&#xA;- We can write this audit log to a file inside of the kind docker container.&#xD;&#xA;&#xD;&#xA;**Describe alternatives you&#39;ve considered**&#xD;&#xA;&#xD;&#xA;- None.&#xD;&#xA;&#xD;&#xA;**Are you considering submitting a PR for this feature?**&#xD;&#xA;&#xD;&#xA;- **How will this project improvement be tested?**&#xD;&#xA;- Manually checking that audit logs are being populated after this fix goes in.&#xD;&#xA;- **How does this change the current architecture?**&#xD;&#xA;- It doesn&#39;t change our source code architecture, as it is a test change.&#xD;&#xA;- It will fill up our kind cluster disks more quickly, but these disks are ephemeral as they are inside of the kind container.&#xD;&#xA;- **How will this change be backwards compatible?**&#xD;&#xA;- Yes - this is a purely additive test change.&#xD;&#xA;- **How will this feature be documented?**&#xD;&#xA;- Perhaps we should have some sort of &#34;how to debug test PR test failures&#34; section in our `CONTRIBUTING.md`?&#xD;&#xA;&#xD;&#xA;**Additional context**&#xD;&#xA;Here is what @enj and I did to enable audit logs in one of our kind clusters.&#xD;&#xA;1. SSH into the VM on which our test kind cluster was running.&#xD;&#xA;2. Exec into the kind container.&#xD;&#xA;3. `cd /etc/kubernetes`&#xD;&#xA;4. Create an `audit-policy.yaml` file, something like the below.&#xD;&#xA;```yaml&#xD;&#xA;apiVersion: audit.k8s.io/v1beta1&#xD;&#xA;kind: Policy&#xD;&#xA;metadata:&#xD;&#xA;  name: Default&#xD;&#xA;# Don&#39;t generate audit events for all requests in RequestReceived stage.&#xD;&#xA;omitStages:&#xD;&#xA;- &#34;RequestReceived&#34;&#xD;&#xA;rules:&#xD;&#xA;# Don&#39;t log requests for events&#xD;&#xA;- level: None&#xD;&#xA;  resources:&#xD;&#xA;  - group: &#34;&#34;&#xD;&#xA;    resources: [&#34;events&#34;]&#xD;&#xA;# Don&#39;t log authenticated requests to certain non-resource URL paths.&#xD;&#xA;- level: None&#xD;&#xA;  userGroups: [&#34;system:authenticated&#34;, &#34;system:unauthenticated&#34;]&#xD;&#xA;  nonResourceURLs:&#xD;&#xA;  - &#34;/api*&#34; # Wildcard matching.&#xD;&#xA;  - &#34;/version&#34;&#xD;&#xA;  - &#34;/healthz&#34;&#xD;&#xA;  - &#34;/readyz&#34;&#xD;&#xA;# A catch-all rule to log all other requests at the Metadata level.&#xD;&#xA;- level: Metadata&#xD;&#xA;  # Long-running requests like watches that fall under this rule will not&#xD;&#xA;  # generate an audit event in RequestReceived.&#xD;&#xA;  omitStages:&#xD;&#xA;  - &#34;RequestReceived&#34;&#xD;&#xA;```&#xD;&#xA;5. Add the `--audit-policy-file=/etc/kubernetes/audit-policy.yaml` flag to the `manifests/kube-apiserver.yaml` `command` array (surely there is a way in `kind` to do this).&#xD;&#xA;6. Add the `--audit-log-path=/var/log/kube-audit.log` flag to the `manifests/kube-apiserver.yaml` `command` array (surely there is a way in `kind` to do this).&#xD;&#xA;7. Add `volumeMounts` and `volumes` for those files (surely there is a way in `kind` to do this).&#xD;&#xA;```yaml&#xD;&#xA;   volumeMounts:&#xD;&#xA;    - mountPath: /var/log&#xD;&#xA;      name: log&#xD;&#xA;    - mountPath: /etc/kubernetes/audit-policy.yaml&#xD;&#xA;      name: audit&#xD;&#xA;      readOnly: true&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;  volumes:&#xD;&#xA;  - hostPath:&#xD;&#xA;      path: /var/log&#xD;&#xA;      type: DirectoryOrCreate&#xD;&#xA;    name: log&#xD;&#xA;  - hostPath:&#xD;&#xA;      path: /etc/kubernetes/audit-policy.yaml&#xD;&#xA;      type: File&#xD;&#xA;    name: audit&#xD;&#xA;```</description>
     <requested_by>ankeesler</requested_by>
     <story_type>feature</story_type>
     <created_at>2021-01-21T16:28:56Z</created_at>
   </external_story>
 </external_stories>
